++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' 3 == 2 ']'
+ '[' 3 == 3 ']'
++ python3 -V
+ pyv3='Python 3.7.3'
+ export PYTHON_VERSION=3.7.3
+ PYTHON_VERSION=3.7.3
+ export PYSPARK_PYTHON=python3
+ PYSPARK_PYTHON=python3
+ export PYSPARK_DRIVER_PYTHON=python3
+ PYSPARK_DRIVER_PYTHON=python3
+ '[' -n '' ']'
+ '[' -z ']'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.1.1.89 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///opt/spark/data/App.py
20/12/13 17:37:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/12/13 17:37:21 INFO SparkContext: Running Spark version 3.0.1
20/12/13 17:37:21 INFO ResourceUtils: ==============================================================
20/12/13 17:37:21 INFO ResourceUtils: Resources for spark.driver:

20/12/13 17:37:21 INFO ResourceUtils: ==============================================================
20/12/13 17:37:21 INFO SparkContext: Submitted application: UginDGCNN
20/12/13 17:37:21 INFO SecurityManager: Changing view acls to: 185,konstantinigin
20/12/13 17:37:21 INFO SecurityManager: Changing modify acls to: 185,konstantinigin
20/12/13 17:37:21 INFO SecurityManager: Changing view acls groups to: 
20/12/13 17:37:21 INFO SecurityManager: Changing modify acls groups to: 
20/12/13 17:37:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(185, konstantinigin); groups with view permissions: Set(); users  with modify permissions: Set(185, konstantinigin); groups with modify permissions: Set()
20/12/13 17:37:22 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
20/12/13 17:37:22 INFO SparkEnv: Registering MapOutputTracker
20/12/13 17:37:22 INFO SparkEnv: Registering BlockManagerMaster
20/12/13 17:37:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/13 17:37:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/13 17:37:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/13 17:37:22 INFO DiskBlockManager: Created local directory at /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/blockmgr-a3bf6164-a626-4479-b1e7-d6ffe5d03711
20/12/13 17:37:22 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
20/12/13 17:37:22 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/13 17:37:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/13 17:37:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:4040
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/13 17:37:23 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
20/12/13 17:37:26 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
20/12/13 17:37:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
20/12/13 17:37:26 INFO NettyBlockTransferService: Server created on app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7079
20/12/13 17:37:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/13 17:37:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, app-py-4b50a0765d2d7d2d-driver-svc.default.svc, 7079, None)
20/12/13 17:37:26 INFO BlockManagerMasterEndpoint: Registering block manager app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, app-py-4b50a0765d2d7d2d-driver-svc.default.svc, 7079, None)
20/12/13 17:37:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, app-py-4b50a0765d2d7d2d-driver-svc.default.svc, 7079, None)
20/12/13 17:37:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, app-py-4b50a0765d2d7d2d-driver-svc.default.svc, 7079, None)
20/12/13 17:37:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/13 17:37:35 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.1.91:52356) with ID 2
20/12/13 17:37:35 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.1.90:58392) with ID 1
20/12/13 17:37:36 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.1.91:44551 with 413.9 MiB RAM, BlockManagerId(2, 10.1.1.91, 44551, None)
20/12/13 17:37:36 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.1.90:35535 with 413.9 MiB RAM, BlockManagerId(1, 10.1.1.90, 35535, None)
20/12/13 17:37:56 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/13 17:37:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
20/12/13 17:37:56 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2020-12-13 17:37:58,446 - Node 46 - INFO - Spark Context established, going though app logic...
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/dependencies.zip at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/dependencies.zip with timestamp 1607881078546
20/12/13 17:37:58 INFO Utils: /opt/spark/data/dependencies.zip has been previously copied to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/dependencies.zip
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/models/USAir_hyper.pkl at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/USAir_hyper.pkl with timestamp 1607881078617
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/models/USAir_hyper.pkl to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/USAir_hyper.pkl
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/models/USAir_model.pth at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/USAir_model.pth with timestamp 1607881078636
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/models/USAir_model.pth to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/USAir_model.pth
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/dll/libgnn.d at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/libgnn.d with timestamp 1607881078651
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/dll/libgnn.d to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/libgnn.d
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/dll/libgnn.so at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/libgnn.so with timestamp 1607881078669
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/dll/libgnn.so to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/libgnn.so
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/config.d at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/config.d with timestamp 1607881078683
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/config.d to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/config.d
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/config.o at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/config.o with timestamp 1607881078703
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/config.o to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/config.o
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/graph_struct.d at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/graph_struct.d with timestamp 1607881078732
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/graph_struct.d to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/graph_struct.d
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/graph_struct.o at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/graph_struct.o with timestamp 1607881078746
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/graph_struct.o to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/graph_struct.o
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/msg_pass.d at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/msg_pass.d with timestamp 1607881078762
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/msg_pass.d to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/msg_pass.d
20/12/13 17:37:58 INFO SparkContext: Added file /opt/spark/data/build/lib/msg_pass.o at spark://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7078/files/msg_pass.o with timestamp 1607881078788
20/12/13 17:37:58 INFO Utils: Copying /opt/spark/data/build/lib/msg_pass.o to /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/userFiles-f042212c-ede3-4b1a-a40a-a4936ca268ec/msg_pass.o
2020-12-13 17:37:58,799 - Node 46 - INFO - Build paths attached...
20/12/13 17:37:59 INFO SparkContext: Starting job: collect at /opt/spark/data/App.py:210
20/12/13 17:37:59 INFO DAGScheduler: Got job 0 (collect at /opt/spark/data/App.py:210) with 2 output partitions
20/12/13 17:37:59 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /opt/spark/data/App.py:210)
20/12/13 17:37:59 INFO DAGScheduler: Parents of final stage: List()
20/12/13 17:37:59 INFO DAGScheduler: Missing parents: List()
20/12/13 17:37:59 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at collect at /opt/spark/data/App.py:210), which has no missing parents
20/12/13 17:38:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.8 KiB, free 413.9 MiB)
20/12/13 17:38:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 413.9 MiB)
20/12/13 17:38:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on app-py-4b50a0765d2d7d2d-driver-svc.default.svc:7079 (size: 5.3 KiB, free: 413.9 MiB)
20/12/13 17:38:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
20/12/13 17:38:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at collect at /opt/spark/data/App.py:210) (first 15 tasks are for partitions Vector(0, 1))
20/12/13 17:38:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
20/12/13 17:38:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.1.1.91, executor 2, partition 0, PROCESS_LOCAL, 8685 bytes)
20/12/13 17:38:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.1.1.90, executor 1, partition 1, PROCESS_LOCAL, 8869 bytes)
20/12/13 17:38:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.1.91:44551 (size: 5.3 KiB, free: 413.9 MiB)
20/12/13 17:38:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.1.90:35535 (size: 5.3 KiB, free: 413.9 MiB)
20/12/13 17:47:09 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/13 17:47:09 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 2, 10.1.1.90, executor 1, partition 1, PROCESS_LOCAL, 8869 bytes)
20/12/13 17:49:16 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 10.1.1.91, executor 2): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)
	... 29 more

20/12/13 17:49:16 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 3, 10.1.1.91, executor 2, partition 0, PROCESS_LOCAL, 8685 bytes)
20/12/13 17:56:07 WARN TaskSetManager: Lost task 1.1 in stage 0.0 (TID 2, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/13 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 4, 10.1.1.90, executor 1, partition 1, PROCESS_LOCAL, 8869 bytes)
20/12/13 17:58:57 WARN TaskSetManager: Lost task 0.1 in stage 0.0 (TID 3, 10.1.1.91, executor 2): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)
	... 29 more

20/12/13 17:58:57 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 5, 10.1.1.91, executor 2, partition 0, PROCESS_LOCAL, 8685 bytes)
20/12/13 18:03:28 WARN TaskSetManager: Lost task 1.2 in stage 0.0 (TID 4, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/13 18:03:28 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 6, 10.1.1.90, executor 1, partition 1, PROCESS_LOCAL, 8869 bytes)
20/12/13 18:07:50 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
io.fabric8.kubernetes.client.KubernetesClientException: The resourceVersion for the provided watch is too old.
	at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onMessage(WatchConnectionManager.java:259)
	at okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)
	at okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)
	at okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)
	at okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/12/13 18:08:31 WARN TaskSetManager: Lost task 0.2 in stage 0.0 (TID 5, 10.1.1.91, executor 2): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)
	... 29 more

20/12/13 18:08:31 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 7, 10.1.1.91, executor 2, partition 0, PROCESS_LOCAL, 8685 bytes)
20/12/13 18:11:41 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 6, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/12/13 18:11:41 ERROR TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job
20/12/13 18:11:41 INFO TaskSchedulerImpl: Cancelling stage 0
20/12/13 18:11:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
20/12/13 18:11:41 INFO TaskSchedulerImpl: Stage 0 was cancelled
20/12/13 18:11:41 INFO DAGScheduler: ResultStage 0 (collect at /opt/spark/data/App.py:210) failed in 2021.512 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 6, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/13 18:11:41 INFO DAGScheduler: Job 0 failed: collect at /opt/spark/data/App.py:210, took 2024.156414 s
Traceback (most recent call last):
  File "/opt/spark/data/App.py", line 217, in <module>
    main()
  File "/opt/spark/data/App.py", line 210, in main
    results = predictions.collect()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 889, in collect
  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 128, in deco
  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 6, 10.1.1.90, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 597, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 271, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/data/App.py", line 207, in <lambda>
    predictions = prediction_subgraphs.map(lambda graph: apply_network(dataset, graph))
  File "/opt/spark/data/App.py", line 115, in apply_network
    return predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 42, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 117, in forward
    feature_label = self.PrepareFeatureLabel(batch_graph)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 88, in PrepareFeatureLabel
    node_tag.scatter_(1, concat_tag, 1)
RuntimeError: index 17 is out of bounds for dimension 1 with size 16

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

20/12/13 18:11:41 INFO SparkContext: Invoking stop() from shutdown hook
20/12/13 18:11:41 INFO SparkUI: Stopped Spark web UI at http://app-py-4b50a0765d2d7d2d-driver-svc.default.svc:4040
20/12/13 18:11:41 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
20/12/13 18:11:41 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
20/12/13 18:11:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/13 18:11:42 INFO MemoryStore: MemoryStore cleared
20/12/13 18:11:42 INFO BlockManager: BlockManager stopped
20/12/13 18:11:42 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/13 18:11:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/13 18:11:42 INFO SparkContext: Successfully stopped SparkContext
20/12/13 18:11:42 INFO ShutdownHookManager: Shutdown hook called
20/12/13 18:11:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-1165e395-36f5-4b8d-a41f-296756abfb59
20/12/13 18:11:42 INFO ShutdownHookManager: Deleting directory /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc
20/12/13 18:11:42 INFO ShutdownHookManager: Deleting directory /var/data/spark-4f58e0e9-9ad0-4e9c-96f7-e565f3e23c0f/spark-73648e9b-b3a3-41a7-a9d4-ea925d334abc/pyspark-8dcede8e-5c8c-49e9-b8c7-65b4cace1bb6
