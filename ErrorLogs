++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ sort -t_ -k4 -n
+ grep SPARK_JAVA_OPT_
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' 3 == 2 ']'
+ '[' 3 == 3 ']'
++ python3 -V
+ pyv3='Python 3.7.3'
+ export PYTHON_VERSION=3.7.3
+ PYTHON_VERSION=3.7.3
+ export PYSPARK_PYTHON=python3
+ PYSPARK_PYTHON=python3
+ export PYSPARK_DRIVER_PYTHON=python3
+ PYSPARK_DRIVER_PYTHON=python3
+ '[' -n '' ']'
+ '[' -z ']'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=10.1.0.148 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///opt/spark/data/ApplyNetwork.py
20/12/01 13:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/12/01 13:50:55 INFO SparkContext: Running Spark version 3.0.1
20/12/01 13:50:55 INFO ResourceUtils: ==============================================================
20/12/01 13:50:55 INFO ResourceUtils: Resources for spark.driver:

20/12/01 13:50:55 INFO ResourceUtils: ==============================================================
20/12/01 13:50:55 INFO SparkContext: Submitted application: UginDGCNN
20/12/01 13:50:56 INFO SecurityManager: Changing view acls to: 185,konstantinigin
20/12/01 13:50:56 INFO SecurityManager: Changing modify acls to: 185,konstantinigin
20/12/01 13:50:56 INFO SecurityManager: Changing view acls groups to: 
20/12/01 13:50:56 INFO SecurityManager: Changing modify acls groups to: 
20/12/01 13:50:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(185, konstantinigin); groups with view permissions: Set(); users  with modify permissions: Set(185, konstantinigin); groups with modify permissions: Set()
20/12/01 13:50:56 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
20/12/01 13:50:56 INFO SparkEnv: Registering MapOutputTracker
20/12/01 13:50:57 INFO SparkEnv: Registering BlockManagerMaster
20/12/01 13:50:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/01 13:50:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/01 13:50:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/01 13:50:57 INFO DiskBlockManager: Created local directory at /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/blockmgr-8e7316c3-82e3-4213-95ef-16d7f9e57d61
20/12/01 13:50:57 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
20/12/01 13:50:57 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/01 13:50:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/01 13:50:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:4040
20/12/01 13:50:58 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/01 13:50:58 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/01 13:50:58 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/01 13:50:58 WARN SparkContext: File with 'local' scheme is not supported to add to file server, since it is already available on every node.
20/12/01 13:50:58 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
20/12/01 13:51:01 INFO ExecutorPodsAllocator: Going to request 5 executors from Kubernetes.
20/12/01 13:51:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
20/12/01 13:51:01 INFO NettyBlockTransferService: Server created on applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7079
20/12/01 13:51:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/01 13:51:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, applynetwork-py-ddc819761e91df9b-driver-svc.default.svc, 7079, None)
20/12/01 13:51:01 INFO BlockManagerMasterEndpoint: Registering block manager applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7079 with 413.9 MiB RAM, BlockManagerId(driver, applynetwork-py-ddc819761e91df9b-driver-svc.default.svc, 7079, None)
20/12/01 13:51:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, applynetwork-py-ddc819761e91df9b-driver-svc.default.svc, 7079, None)
20/12/01 13:51:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, applynetwork-py-ddc819761e91df9b-driver-svc.default.svc, 7079, None)
20/12/01 13:51:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
20/12/01 13:51:08 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.0.150:50932) with ID 2
20/12/01 13:51:08 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.0.149:33742) with ID 1
20/12/01 13:51:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.0.150:43549 with 413.9 MiB RAM, BlockManagerId(2, 10.1.0.150, 43549, None)
20/12/01 13:51:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.0.149:33465 with 413.9 MiB RAM, BlockManagerId(1, 10.1.0.149, 33465, None)
20/12/01 13:51:30 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
20/12/01 13:51:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
20/12/01 13:51:31 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
2020-12-01 13:51:32,841 - Node 44 - INFO - Spark Context established, going though app logic...
20/12/01 13:51:32 INFO SparkContext: Added file /opt/spark/data/dependencies.zip at spark://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7078/files/dependencies.zip with timestamp 1606830692915
20/12/01 13:51:32 INFO Utils: /opt/spark/data/dependencies.zip has been previously copied to /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b/userFiles-8a94cbf8-419d-4f0d-894f-a9409cb9f7db/dependencies.zip
20/12/01 13:51:32 INFO SparkContext: Added file /opt/spark/data/models/USAir_hyper.pkl at spark://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7078/files/USAir_hyper.pkl with timestamp 1606830692972
20/12/01 13:51:32 INFO Utils: Copying /opt/spark/data/models/USAir_hyper.pkl to /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b/userFiles-8a94cbf8-419d-4f0d-894f-a9409cb9f7db/USAir_hyper.pkl
20/12/01 13:51:32 INFO SparkContext: Added file /opt/spark/data/models/USAir_model.pth at spark://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7078/files/USAir_model.pth with timestamp 1606830692993
20/12/01 13:51:32 INFO Utils: Copying /opt/spark/data/models/USAir_model.pth to /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b/userFiles-8a94cbf8-419d-4f0d-894f-a9409cb9f7db/USAir_model.pth
20/12/01 13:51:33 INFO SparkContext: Added file /opt/spark/data/binaries.zip at spark://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7078/files/binaries.zip with timestamp 1606830693010
20/12/01 13:51:33 INFO Utils: Copying /opt/spark/data/binaries.zip to /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b/userFiles-8a94cbf8-419d-4f0d-894f-a9409cb9f7db/binaries.zip
20/12/01 13:51:33 INFO SparkContext: Starting job: foreach at /opt/spark/data/ApplyNetwork.py:64
20/12/01 13:51:33 INFO DAGScheduler: Got job 0 (foreach at /opt/spark/data/ApplyNetwork.py:64) with 2 output partitions
20/12/01 13:51:33 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at /opt/spark/data/ApplyNetwork.py:64)
20/12/01 13:51:33 INFO DAGScheduler: Parents of final stage: List()
20/12/01 13:51:33 INFO DAGScheduler: Missing parents: List()
20/12/01 13:51:33 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at foreach at /opt/spark/data/ApplyNetwork.py:64), which has no missing parents
20/12/01 13:51:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.7 KiB, free 413.9 MiB)
20/12/01 13:51:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 413.9 MiB)
20/12/01 13:51:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:7079 (size: 4.2 KiB, free: 413.9 MiB)
20/12/01 13:51:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
20/12/01 13:51:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at foreach at /opt/spark/data/ApplyNetwork.py:64) (first 15 tasks are for partitions Vector(0, 1))
20/12/01 13:51:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
20/12/01 13:51:34 WARN TaskSetManager: Stage 0 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.
20/12/01 13:51:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.1.0.149, executor 1, partition 0, PROCESS_LOCAL, 1451383 bytes)
20/12/01 13:51:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.1.0.150, executor 2, partition 1, PROCESS_LOCAL, 1694686 bytes)
20/12/01 13:51:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.0.150:43549 (size: 4.2 KiB, free: 413.9 MiB)
20/12/01 13:51:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.0.149:33465 (size: 4.2 KiB, free: 413.9 MiB)
20/12/01 13:51:39 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 10.1.0.150, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'

  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
  at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
  at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
  at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
  at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
  at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)

20/12/01 13:51:39 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 2, 10.1.0.149, executor 1, partition 1, PROCESS_LOCAL, 1694686 bytes)
20/12/01 13:51:39 INFO TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) on 10.1.0.149, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 1]
20/12/01 13:51:39 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 3, 10.1.0.150, executor 2, partition 0, PROCESS_LOCAL, 1451383 bytes)
20/12/01 13:51:40 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 3) on 10.1.0.150, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 2]
20/12/01 13:51:40 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 4, 10.1.0.150, executor 2, partition 0, PROCESS_LOCAL, 1451383 bytes)
20/12/01 13:51:40 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 2) on 10.1.0.149, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 3]
20/12/01 13:51:40 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 5, 10.1.0.149, executor 1, partition 1, PROCESS_LOCAL, 1694686 bytes)
20/12/01 13:51:41 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 4) on 10.1.0.150, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 4]
20/12/01 13:51:41 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 6, 10.1.0.150, executor 2, partition 0, PROCESS_LOCAL, 1451383 bytes)
20/12/01 13:51:41 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) on 10.1.0.149, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 5]
20/12/01 13:51:41 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 7, 10.1.0.149, executor 1, partition 1, PROCESS_LOCAL, 1694686 bytes)
20/12/01 13:51:42 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) on 10.1.0.150, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'
) [duplicate 6]
20/12/01 13:51:42 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
20/12/01 13:51:42 INFO TaskSchedulerImpl: Cancelling stage 0
20/12/01 13:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
20/12/01 13:51:42 INFO TaskSchedulerImpl: Stage 0 was cancelled
20/12/01 13:51:42 INFO DAGScheduler: ResultStage 0 (foreach at /opt/spark/data/ApplyNetwork.py:64) failed in 8.922 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, 10.1.0.150, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'

  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
  at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
  at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
  at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
  at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
  at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
20/12/01 13:51:42 INFO DAGScheduler: Job 0 failed: foreach at /opt/spark/data/ApplyNetwork.py:64, took 9.057104 s
20/12/01 13:51:43 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7, 10.1.0.149, executor 1): TaskKilled (Stage cancelled)
20/12/01 13:51:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Traceback (most recent call last):
  File "/opt/spark/data/ApplyNetwork.py", line 68, in <module>
    main()
  File "/opt/spark/data/ApplyNetwork.py", line 64, in main
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 862, in foreach
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1141, in count
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1132, in sum
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 1003, in fold
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 889, in collect
  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 128, in deco
    if max_nodes_per_hop is not None:
  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, 10.1.0.150, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'

  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
  at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
  at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
  at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
  at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
  at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)
  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)
  at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)
  at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  at py4j.Gateway.invoke(Gateway.java:282)
  at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  at py4j.commands.CallCommand.execute(CallCommand.java:79)
  at py4j.GatewayConnection.run(GatewayConnection.java:238)
  at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 605, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 595, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 2596, in pipeline_func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 425, in func
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 860, in processPartition
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 107, in wrapper
  File "/opt/spark/data/ApplyNetwork.py", line 64, in <lambda>
    graphs_rdd.foreach(lambda graph: apply_network(graph))
  File "/opt/spark/data/ApplyNetwork.py", line 25, in apply_network
    predictor.predict(serialized)
  File "./dependencies.zip/pytorch_DGCNN/predictor.py", line 40, in predict
    predictions.append(self.classifier(batch_data)[0][:, 1].exp().cpu().detach())
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/main.py", line 123, in forward
    embed = self.gnn(batch_graph, node_feat, edge_feat)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "./dependencies.zip/pytorch_DGCNN/DGCNN_embedding.py", line 58, in forward
    n2n_sp, e2n_sp, subg_sp = GNNLIB().PrepareSparseMatrices(graph_list)
AttributeError: 'NoneType' object has no attribute 'PrepareSparseMatrices'

  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
  at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
  at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
  at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
  at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
  at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
  at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
  at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)
  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  ... 1 more

20/12/01 13:51:43 INFO SparkContext: Invoking stop() from shutdown hook
20/12/01 13:51:43 INFO SparkUI: Stopped Spark web UI at http://applynetwork-py-ddc819761e91df9b-driver-svc.default.svc:4040
20/12/01 13:51:43 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
20/12/01 13:51:43 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
20/12/01 13:51:43 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
20/12/01 13:51:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/01 13:51:44 INFO MemoryStore: MemoryStore cleared
20/12/01 13:51:44 INFO BlockManager: BlockManager stopped
20/12/01 13:51:44 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/01 13:51:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/01 13:51:44 INFO SparkContext: Successfully stopped SparkContext
20/12/01 13:51:44 INFO ShutdownHookManager: Shutdown hook called
20/12/01 13:51:44 INFO ShutdownHookManager: Deleting directory /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b/pyspark-30f326f3-762c-4156-b947-96fc0fba8289
20/12/01 13:51:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-a93e1323-b12b-4488-9e0e-2fa7c6ce79ab
20/12/01 13:51:44 INFO ShutdownHookManager: Deleting directory /var/data/spark-084042bd-5c3a-43bf-bdb0-5ce01668127a/spark-9f621157-be1a-4a19-be7d-5a45b06d601b
